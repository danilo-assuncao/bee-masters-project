{"cells":[{"cell_type":"markdown","metadata":{"id":"halikPU_KsAj"},"source":["# Model\n","Pipeline that defines the model used to training and test the model specified."]},{"cell_type":"markdown","metadata":{"id":"MjYYJe-GLX_F"},"source":["# Download the dataset zip file\n","This section focus on the download do dataset to local machine."]},{"cell_type":"markdown","metadata":{"id":"vf-rt55SLygt"},"source":["Cleaning temporary directories"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hKhw-ylwL0UU"},"outputs":[],"source":["%rm -rf './sample_data' 'dataset' 'execution-logs' 'checkpoint' 'history.json'"]},{"cell_type":"markdown","metadata":{"id":"xqQJG1ZIQepe"},"source":["Importing dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V7wcR-W-QgOC"},"outputs":[],"source":["from google.colab import drive\n","from zipfile import ZipFile\n","from glob import glob\n","from uuid import uuid4\n","from tqdm import tqdm\n","from psutil import cpu_count, cpu_stats, virtual_memory\n","from tensorflow.keras import backend\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n","from tensorflow.keras.callbacks import LearningRateScheduler\n","from tensorflow.keras.applications import InceptionResNetV2, VGG16, VGG19, ResNet152, InceptionV3\n","from matplotlib import pyplot as plt\n","from sklearn.metrics import ConfusionMatrixDisplay\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import shutil\n","import json\n","import os\n","import re"]},{"cell_type":"markdown","metadata":{"id":"rAXL4vDJLgLE"},"source":["Defining the global constants"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S81dgny8Ledq"},"outputs":[],"source":["class Constants(object):\n","  LOG_ENABLED = True\n","  LEARNING_RATE_DECAY = True\n","  LOG_PATH = './execution-logs'\n","  DRIVE_MOUNT_PATH = './gdrive'\n","  DRIVE_PATH = f'{DRIVE_MOUNT_PATH}/My Drive/Colab Notebooks'\n","  ZIP_DATASET_DRIVE_PATH = f'{DRIVE_PATH}/augmented-dataset.zip'\n","  DRIVE_CHECKPOINTS_PATH = f'{DRIVE_PATH}/checkpoints'\n","  DATASET_PATH = './dataset'\n","  TRAINING_DATASET_PATH = f'{DATASET_PATH}/train'\n","  VALIDATION_DATASET_PATH = f'{DATASET_PATH}/val'\n","  TEST_DATASET_PATH = f'{DATASET_PATH}/test'\n","  BATCH_SIZE = 16\n","  IMAGE_HEIGHT = 224\n","  IMAGE_WIDTH = 224\n","  CHANNEL_SIZE = 3\n","  IMAGE_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, CHANNEL_SIZE)\n","  SEED = 133"]},{"cell_type":"markdown","metadata":{"id":"wVZS9eaQL3mb"},"source":["Log hardware specifications in `execution-logs` folder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OxKe5n0uMAQY"},"outputs":[],"source":["def __log_hardware_info():\n","  from psutil import cpu_count, cpu_stats, virtual_memory\n","  import os\n","\n","  # Create hardware-info directory\n","  if not os.path.exists(Constants.LOG_PATH):\n","    os.mkdir(Constants.LOG_PATH)\n","\n","  # Log cpu informations\n","  %cat /proc/cpuinfo > './execution-logs/cpu.log'\n","\n","  # Log general informations\n","  general_info = {\n","      'cpu_count': cpu_count(),\n","      'cpu_stats': cpu_stats(),\n","      'virtual_memory': virtual_memory()\n","  }\n","  with open(f'{Constants.LOG_PATH}/general.log', 'w') as file:\n","    file.write(str(general_info))\n","\n","if Constants.LOG_ENABLED:\n","  __log_hardware_info()"]},{"cell_type":"markdown","metadata":{"id":"SLxon4MYQCqU"},"source":["Mounting Google Drive in this machine and copy the dataset folder from Google Drive mounted to current path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EnIjwhdCQFEq"},"outputs":[],"source":["drive.mount(Constants.DRIVE_MOUNT_PATH, force_remount=True, timeout_ms=60000)"]},{"cell_type":"markdown","metadata":{"id":"9hVIwy3mQpJ8"},"source":["Copying from Google Drive the dataset to current path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9T5rsFd9QrRT"},"outputs":[],"source":["zip_ref = ZipFile(Constants.ZIP_DATASET_DRIVE_PATH, 'r')\n","zip_ref.extractall(Constants.DATASET_PATH)\n","zip_ref.close()"]},{"cell_type":"markdown","metadata":{"id":"vFpLquasVIGm"},"source":["# Create and configure the model\n","\n","Section dedicated to create and configure the CNN model."]},{"cell_type":"markdown","metadata":{"id":"_V_8cZW_Gtlw"},"source":["Define the training and validation dataset divisions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FKS8BereVSLy"},"outputs":[],"source":["def normalize_data(dataset):\n","  # Pixel values are now in [min 0, max 1]\n","  normalization_layer = tf.keras.layers.Rescaling(1./255)\n","  return dataset.map(lambda x, y: (normalization_layer(x), y))\n","\n","def get_training_dataset():\n","  return tf.keras.utils.image_dataset_from_directory(\n","      Constants.TRAINING_DATASET_PATH,\n","      label_mode='categorical',\n","      color_mode='rgb',\n","      shuffle=True,\n","      seed=Constants.SEED,\n","      image_size=(Constants.IMAGE_HEIGHT, Constants.IMAGE_WIDTH),\n","      batch_size=Constants.BATCH_SIZE\n","  )\n","\n","def get_validation_dataset():\n","  return tf.keras.utils.image_dataset_from_directory(\n","      Constants.VALIDATION_DATASET_PATH,\n","      label_mode='categorical',\n","      color_mode='rgb',\n","      shuffle=True,\n","      seed=Constants.SEED,\n","      image_size=(Constants.IMAGE_HEIGHT, Constants.IMAGE_WIDTH),\n","      batch_size=Constants.BATCH_SIZE\n","  )\n","\n","def get_test_dataset():\n","  return tf.keras.utils.image_dataset_from_directory(\n","      Constants.TEST_DATASET_PATH,\n","      label_mode='categorical',\n","      color_mode='rgb',\n","      shuffle=True,\n","      seed=Constants.SEED,\n","      image_size=(Constants.IMAGE_HEIGHT, Constants.IMAGE_WIDTH),\n","  )\n","\n","# Training dataset\n","training_dataset = get_training_dataset()\n","training_dataset_normalized = normalize_data(training_dataset)\n","\n","# Validation dataset\n","validation_dataset = get_validation_dataset()\n","\n","# Test dataset\n","test_dataset = get_test_dataset()\n","\n","# Number of classes\n","number_class = len(training_dataset.class_names)"]},{"cell_type":"markdown","metadata":{"id":"SzeMijp5Qdo8"},"source":["[Tune datasets](https://www.tensorflow.org/guide/data_performance) applying caching"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1uHi7-lqQh71"},"outputs":[],"source":["AUTOTUNE = tf.data.AUTOTUNE\n","training_dataset = training_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n","validation_dataset = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"-7QD59Svf9q_"},"source":["Define the model architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e7ONsnZugk-5"},"outputs":[],"source":["\"\"\"\n","INCEPTION RESNET V2\n","\"\"\"\n","def inception_resnet_v2_model():\n","  model = InceptionResNetV2(input_shape=Constants.IMAGE_SHAPE, \n","                            weights=None, \n","                            classes=number_class)\n","  model.summary()\n","  return model\n","\n","def inception_resnet_v2_pretrained_model():\n","  model = InceptionResNetV2(input_shape=Constants.IMAGE_SHAPE, \n","                            include_top=False, \n","                            weights='imagenet', \n","                            classes=number_class)\n","  \n","  x = GlobalAveragePooling2D(name='avg_pool')(model.output)\n","  x = Dense(number_class, activation='softmax', name='predictions')(x)\n","\n","  model = Model(model.input, x)\n","  model.summary()\n","  return model\n","\n","\"\"\"\n","VGG16\n","\"\"\"\n","def vgg_16_model():\n","  model = VGG16(input_shape=Constants.IMAGE_SHAPE, \n","                weights=None, \n","                classes=number_class)\n","  model.summary()\n","  return model\n","\n","def vgg_16_pretrained_model():\n","  model = VGG16(input_shape=Constants.IMAGE_SHAPE, \n","                include_top=False, \n","                weights='imagenet', \n","                classes=number_class)\n","  \n","  x = GlobalAveragePooling2D(name='avg_pool')(model.output)\n","  x = Dense(number_class, activation='softmax', name='predictions')(x)\n","\n","  model = Model(model.input, x)\n","  model.summary()\n","  return model\n","\n","\"\"\"\n","VGG19\n","\"\"\"\n","def vgg_19_model():\n","  model = VGG19(input_shape=Constants.IMAGE_SHAPE, \n","                weights=None, \n","                classes=number_class)\n","  \n","  model.summary()\n","  return model\n","\n","def vgg_19_pretrained_model():\n","  model = VGG19(input_shape=Constants.IMAGE_SHAPE, \n","                include_top=False, \n","                weights='imagenet', \n","                classes=number_class)\n","  \n","  x = GlobalAveragePooling2D(name='avg_pool')(model.output)\n","  x = Dense(number_class, activation='softmax', name='predictions')(x)\n","\n","  model = Model(model.input, x)\n","  model.summary()\n","  return model\n","\n","\"\"\"\n","RESNET 152 \n","\"\"\"\n","def resnet_152_model():\n","  model = ResNet152(input_shape=Constants.IMAGE_SHAPE, \n","                    weights=None, \n","                    classes=number_class)\n","  model.summary()\n","  return model\n","\n","def resnet_152_pretrained_model():\n","  model = ResNet152(input_shape=Constants.IMAGE_SHAPE, \n","                    include_top=False, \n","                    weights='imagenet', \n","                    classes=number_class)\n","  \n","  x = GlobalAveragePooling2D(name='avg_pool')(model.output)\n","  x = Dense(number_class, activation='softmax', name='predictions')(x)\n","\n","  model = Model(model.input, x)\n","  model.summary()\n","  return model\n","\n","\"\"\"\n","INCEPTION V3 \n","\"\"\"\n","def inception_v3_model():\n","  model = InceptionV3(input_shape=Constants.IMAGE_SHAPE, \n","                      weights=None, \n","                      classes=number_class)\n","  model.summary()\n","  return model\n","\n","def inception_v3_pretrained_model():\n","  model = InceptionV3(input_shape=Constants.IMAGE_SHAPE, \n","                      include_top=False, \n","                      weights='imagenet', \n","                      classes=number_class)\n","  \n","  x = GlobalAveragePooling2D(name='avg_pool')(model.output)\n","  x = Dense(number_class, activation='softmax', name='predictions')(x)\n","\n","  model = Model(model.input, x)\n","  model.summary()\n","  return model"]},{"cell_type":"markdown","metadata":{"id":"2A4MuhUXGsSZ"},"source":["Define training callbacks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8e6MOmgGv2I"},"outputs":[],"source":["class TimestampCallback(tf.keras.callbacks.Callback):\n","  def on_epoch_begin(self, epoch, logs=None):\n","    import datetime\n","    self.__epoch_start = datetime.datetime.utcnow()\n","\n","  def on_epoch_end(self, epoch, logs=None):\n","    import datetime\n","    timedelta_sec = datetime.datetime.utcnow() - self.__epoch_start\n","    logs[\"duration\"] = timedelta_sec.total_seconds()\n","\n","def learning_rate_decay_scheduler(epoch):\n","  learning_rate = 0.001\n","  if Constants.LEARNING_RATE_DECAY:\n","    if epoch > 40:\n","      learning_rate = 0.0005\n","    if epoch > 50:\n","      learning_rate = 0.0001\n","  return learning_rate\n","\n","# Define callbacks\n","callbacks = [LearningRateScheduler(learning_rate_decay_scheduler), TimestampCallback()]"]},{"cell_type":"markdown","source":["# Model Definitions\n","Section dedicated to apply some definition in the model."],"metadata":{"id":"58MRbokB9bLm"}},{"cell_type":"markdown","metadata":{"id":"J3u77_2JKzPJ"},"source":["Configure constants for model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zcmPvSfVK3m1"},"outputs":[],"source":["class ModelConstants(object):\n","  \"\"\"\n","  Currently algorithm models available.\n","  * inception_resnet_v2_model\n","  * inception_resnet_v2_pretrained_model\n","  * vgg_16_model\n","  * vgg_16_pretrained_model\n","  * vgg_19_model\n","  * vgg_19_pretrained_model\n","  * resnet_152_model\n","  * resnet_152_pretrained_model\n","  * inception_v3_model\n","  * inception_v3_pretrained_model\n","  \"\"\"\n","  MODEL_NAME = 'vgg_16_pretrained_model'\n","\n","  \"\"\"\n","  Training epochs.\n","  \"\"\"\n","  EPOCHS = 8\n","\n","  \"\"\"\n","  Dataset type is used to get the models given the dataset type.\n","  * augmented\n","  * not-augmented\n","  \"\"\"\n","  DATASET_TYPE = 'augmented'\n","\n","  \"\"\"\n","  Test ID used to identifies the test approached.\n","  * algorithm\n","  * epochs\n","  * optimizer\n","  \"\"\"\n","  TEST_TYPE = 'epochs'\n","\n","  \"\"\"\n","  Optimizer and its configurations.\n","  * class_name - Optimizer algorithm (SGD, Adam, Adadelta, Adagrad, Adamax)\n","  * config - General optimizer parameters\n","  \"\"\"\n","  OPTIMIZER = {'class_name': 'SGD', \n","               'config': {'learning_rate': 0.001, \n","                          # 'momentum': 0.9, \n","                          # 'nesterov': True\n","                        }}\n","\n","  \"\"\"\n","  Checkpoint desired to be loaded. The <DATASET_TYPE> and <TEST_TYPE> must be defined \n","  above to this algorithm get the right model.\n","  * <MODEL_DIR_NAME>\n","  * disabled\n","  \"\"\"\n","  CHECKPOINT_NAME = 'disabled'\n","  DRIVE_CHECKPOINT_DIR_PATH = f'{Constants.DRIVE_CHECKPOINTS_PATH}/{DATASET_TYPE}/{TEST_TYPE}'\n","  DRIVE_CHECKPOINT_PATH = f'{DRIVE_CHECKPOINT_DIR_PATH}/{CHECKPOINT_NAME}'"]},{"cell_type":"markdown","source":["# Training and validation\n","Section dedicated to train and validate the model."],"metadata":{"id":"jV7a05T39-lO"}},{"cell_type":"markdown","metadata":{"id":"4YVFDIWsGm-S"},"source":["Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FHTGZ_0hGpU4"},"outputs":[],"source":["def get_model():\n","  model = globals()[ModelConstants.MODEL_NAME]()\n","  model.compile(loss='categorical_crossentropy',\n","                optimizer=tf.keras.optimizers.get(ModelConstants.OPTIMIZER),\n","                metrics=['accuracy', \n","                         'top_k_categorical_accuracy', \n","                         tf.keras.metrics.Precision(name='precision'), \n","                         tf.keras.metrics.Recall(name='recall'),\n","                         tf.keras.metrics.TruePositives(name='true_positives'),\n","                         tf.keras.metrics.TrueNegatives(name='true_negatives'),\n","                         tf.keras.metrics.FalsePositives(name='false_positives'),\n","                         tf.keras.metrics.FalseNegatives(name='false_negatives'),\n","                         tf.keras.metrics.AUC(name='auc')])\n","  return model\n","    \n","def is_model_saved_in_cloud():\n","  return os.path.exists(ModelConstants.DRIVE_CHECKPOINT_PATH)\n","\n","def load_checkpoint_saved():\n","  if is_model_saved_in_cloud():\n","    history = json.load(open(f'{ModelConstants.DRIVE_CHECKPOINT_PATH}/history.json', 'r'))\n","    model = load_model(f'{ModelConstants.DRIVE_CHECKPOINT_PATH}/model')\n","  return model, history\n","\n","def fit(model):\n","  return model.fit( \n","      training_dataset,\n","      validation_data=validation_dataset,\n","      epochs=ModelConstants.EPOCHS,\n","      callbacks=callbacks\n","  )\n","\n","def train_model():\n","  if is_model_saved_in_cloud():\n","    model, history = load_checkpoint_saved()\n","    return model, history\n","  else:\n","    model = get_model()\n","    history = fit(model)\n","    return model, history\n","\n","model, history = train_model()"]},{"cell_type":"markdown","metadata":{"id":"SSp9IUz1ou60"},"source":["Extract data from history object"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kxfBF1xRn7hR"},"outputs":[],"source":["def get_data_from_history():\n","  if hasattr(history, 'history'):\n","    return history.history\n","  return history\n","\n","history = get_data_from_history()"]},{"cell_type":"markdown","metadata":{"id":"coJfIUB75Sz_"},"source":["Define checkpoint constants"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DFKsB4aJ5U-B"},"outputs":[],"source":["def get_checkpoint_name():\n","  validation_accuracy = '{:.2f}'.format(round(history['val_accuracy'][-1]*100, 2))\n","  duration = '{:.0f}'.format(round(sum(history[\"duration\"])/60, 2))\n","  optimizer_name = ModelConstants.OPTIMIZER['class_name'].lower()\n","  return f'{ModelConstants.MODEL_NAME}_{optimizer_name}_{ModelConstants.EPOCHS}epochs_{ModelConstants.OPTIMIZER[\"config\"][\"learning_rate\"]}lr_{validation_accuracy}acc_{duration}min'\n","\n","class CheckpointConstants(object):\n","  CHECKPOINT_NAME = get_checkpoint_name()\n","  DRIVE_CHECKPOINT_PATH = f'{ModelConstants.DRIVE_CHECKPOINT_DIR_PATH}/{CHECKPOINT_NAME}'\n","  DRIVE_CHECKPOINT_MODEL_PATH = f'{DRIVE_CHECKPOINT_PATH}/model'\n","  DRIVE_CHECKPOINT_HISTORY_FILE_PATH = f'{DRIVE_CHECKPOINT_PATH}/history.json'\n","  LOCAL_CHECKPOINT_PATH = './checkpoint'\n","  LOCAL_HISTORY_FILE_PATH = './history.json'"]},{"cell_type":"markdown","metadata":{"id":"H9mY5_8Za0h7"},"source":["Save the model in Google Drive if not exists"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hRTpiZBQa3gP"},"outputs":[],"source":["def save_model_in_root(model):\n","  if os.path.exists(CheckpointConstants.LOCAL_CHECKPOINT_PATH):\n","    shutil.rmtree(CheckpointConstants.LOCAL_CHECKPOINT_PATH)\n","  model.save(CheckpointConstants.LOCAL_CHECKPOINT_PATH, save_format='tf')\n","\n","def copy_model_in_cloud():\n","  if os.path.exists(CheckpointConstants.DRIVE_CHECKPOINT_MODEL_PATH):\n","    shutil.rmtree(CheckpointConstants.DRIVE_CHECKPOINT_MODEL_PATH)\n","  shutil.copytree(CheckpointConstants.LOCAL_CHECKPOINT_PATH, CheckpointConstants.DRIVE_CHECKPOINT_MODEL_PATH)\n","\n","def save_model(model):\n","  save_model_in_root(model)\n","  copy_model_in_cloud()\n","\n","def save_history_in_root(history):\n","  history_logs = history\n","  learning_rates = history_logs['lr']\n","  history_logs['lr'] = list(map(lambda x: float(\"{:.8f}\".format(float(x))), learning_rates))\n","  json.dump(history_logs, open(CheckpointConstants.LOCAL_HISTORY_FILE_PATH, 'w'))\n","\n","def copy_history_in_cloud():\n","  if os.path.exists(CheckpointConstants.DRIVE_CHECKPOINT_HISTORY_FILE_PATH):\n","    os.unlink(CheckpointConstants.DRIVE_CHECKPOINT_HISTORY_FILE_PATH)\n","  shutil.copy(CheckpointConstants.LOCAL_HISTORY_FILE_PATH, CheckpointConstants.DRIVE_CHECKPOINT_HISTORY_FILE_PATH)\n","\n","def save_history(history):\n","  save_history_in_root(history)\n","  copy_history_in_cloud()\n","\n","if not os.path.exists(ModelConstants.DRIVE_CHECKPOINT_PATH):\n","  save_model(model)\n","  save_history(history)"]},{"cell_type":"markdown","metadata":{"id":"ubCuEd4TAEHk"},"source":["Compile tensorflow log and show it"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6H1B-8Uj7eFa"},"outputs":[],"source":["print('\\n- TRAINING METRICS')\n","print(f'loss - last: {history[\"loss\"][-1]}, max: {max(history[\"loss\"])}, min {min(history[\"loss\"])}')\n","print(f'accuracy - last: {history[\"accuracy\"][-1]}, max: {max(history[\"accuracy\"])}, min {min(history[\"accuracy\"])}')\n","\n","print('\\n- VALIDATION METRICS')\n","print(f'val_loss - last: {history[\"val_loss\"][-1]}, max: {max(history[\"val_loss\"])}, min {min(history[\"val_loss\"])}')\n","print(f'val_accuracy - last: {history[\"val_accuracy\"][-1]}, max: {max(history[\"val_accuracy\"])}, min {min(history[\"val_accuracy\"])}')\n","\n","print('\\n- DURATION')\n","print(f'duration - last: {history[\"duration\"][-1]}, max: {max(history[\"duration\"])}, min {min(history[\"duration\"])}')\n","print(f'duration - hours: {sum(history[\"duration\"])/3600}, minutes: {sum(history[\"duration\"])/60}, seconds: {sum(history[\"duration\"])}')\n","\n","print('\\n- THE BEST EPOCH FROM THIS MODEL:', history[\"val_accuracy\"].index(max(history[\"val_accuracy\"])) + 1, '|', max(history[\"val_accuracy\"]))\n","print()"]},{"cell_type":"markdown","metadata":{"id":"zEYgK60fvF7N"},"source":["# Test and Metrics\n","Section dedicated to test and calculate some metrics of this model."]},{"cell_type":"markdown","metadata":{"id":"1gc9pqWDvvmb"},"source":["Model accuracy chart"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eylu4vuWy0T7"},"outputs":[],"source":["def accuracy_chart():\n","  plt.plot(history['accuracy'])\n","  plt.plot(history['val_accuracy'])\n","  plt.title('Acurácia do modelo - Treinamento')\n","  plt.ylabel('acurácia')\n","  plt.xlabel('épocas')\n","  plt.legend(['train', 'val'], loc='upper left')\n","  plt.show()\n","\n","accuracy_chart()"]},{"cell_type":"markdown","metadata":{"id":"59Cf3A7Uvy8E"},"source":["Model loss chart"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PjwDFyvSZVgG"},"outputs":[],"source":["def loss_chart():\n","  plt.plot(history['loss'])\n","  plt.plot(history['val_loss'])\n","  plt.title('Perda do modelo - Treinamento')\n","  plt.ylabel('perda')\n","  plt.xlabel('épocas')\n","  plt.legend(['train', 'val'], loc='upper left')\n","  plt.show()\n","\n","loss_chart()"]},{"cell_type":"markdown","metadata":{"id":"zRXOPUYE4rNd"},"source":["Evaluate the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vEs3PsSBV1Y8"},"outputs":[],"source":["results = model.evaluate(test_dataset, batch_size=Constants.BATCH_SIZE)"]},{"cell_type":"markdown","source":["Calculte F1-Score"],"metadata":{"id":"Y9b1jRDgMn4C"}},{"cell_type":"code","source":["def calcule_f1_score():\n","  precision=results[3]\n","  recall=results[4]\n","  if (precision + recall) > 0:\n","    return 2 * ((precision * recall) / (precision + recall))\n","  else:\n","    return 0\n","\n","calcule_f1_score()"],"metadata":{"id":"Qxj7I8x9MmyF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t7RcRsiPBVIx"},"source":["Predict using test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"48JcXkIuo7Ez"},"outputs":[],"source":["TEST_SET_SIZE = len(glob(f'{Constants.TEST_DATASET_PATH}/**/*.jpg')) + len(glob(f'{Constants.TEST_DATASET_PATH}/**/*.JPG'))\n","\n","def dataset_to_numpy_util(dataset, N):\n","  dataset = dataset.unbatch().batch(N)\n","  for images, labels in dataset:\n","      numpy_images = images.numpy()\n","      numpy_labels = labels.numpy()\n","      break;  \n","  return numpy_images, numpy_labels\n","\n","images, labels = dataset_to_numpy_util(test_dataset, TEST_SET_SIZE)\n","\n","# Predict\n","predictions = model.predict(images)"]},{"cell_type":"markdown","metadata":{"id":"igVULZROBa-X"},"source":["Show prediction result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QYmyAxCn-5IN"},"outputs":[],"source":["CLASSES = test_dataset.class_names\n","\n","def get_class_name_from_label_and_target(prediction_label, expected_label):\n","  one_hot_label = np.argmax(prediction_label, axis=-1)\n","  one_hot_expected_label = np.argmax(expected_label, axis=-1)\n","  correct = (one_hot_label == one_hot_expected_label)\n","  return CLASSES[one_hot_label], correct, (CLASSES[one_hot_expected_label] if not correct else '-')\n","\n","def get_processed_predictions(images, predictions, labels):\n","  results = list()\n","  score = list()\n","  for i, image in enumerate(images):\n","    class_name, correct, should_be = get_class_name_from_label_and_target(predictions[i], labels[i])\n","    results.append([class_name, bool(correct), should_be])\n","    score.append(bool(correct))\n","  return results\n","\n","def show_predictions():\n","  from tabulate import tabulate\n","  results = get_processed_predictions(images, predictions, labels)\n","  results.sort()\n","  print(tabulate(results, headers=['Class name', 'Correct?', 'Shoud be?']))\n","\n","show_predictions()"]},{"cell_type":"markdown","metadata":{"id":"WxOvOR-l4fr1"},"source":["Print in Table (excel) format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c-IHgQZg4l01"},"outputs":[],"source":["def show_results_in_table_format():\n","  results = get_processed_predictions(images, predictions, labels)\n","  table = [','.join(map(str, line)) for line in results]\n","  print(*table, sep='\\n')\n","\n","show_results_in_table_format()"]},{"cell_type":"markdown","metadata":{"id":"hK1EDXo_KN0z"},"source":["Calculate hits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"so83gpwqKRd9"},"outputs":[],"source":["success = round(results[1] * TEST_SET_SIZE)\n","fail = TEST_SET_SIZE - success\n","print(f'Succeeded in {success} images and failed {fail} images')"]},{"cell_type":"markdown","source":["Export confusion matrix to CSV file"],"metadata":{"id":"aQ0xm-UqYBLM"}},{"cell_type":"code","source":["def export_confusion_matrix_to_csv_file():\n","  true_classes = tf.argmax(labels, 1)\n","  predicted_classes = tf.argmax(predictions, 1)\n","  y_true_classes = pd.Series(true_classes, name=\"Actual\")\n","  y_predicted_classes = pd.Series(predicted_classes, name=\"Predicted\")\n","  confusion_matrix = pd.crosstab(y_true_classes, y_predicted_classes)\n","  confusion_matrix.to_csv('confusion_matrix.csv')\n","\n","export_confusion_matrix_to_csv_file()"],"metadata":{"id":"6waM8gzzQLCA"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"model.ipynb","provenance":[],"authorship_tag":"ABX9TyOeV3T6suJFnfxF5E9JVqBR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}